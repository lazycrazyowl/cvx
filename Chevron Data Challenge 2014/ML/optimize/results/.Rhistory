for(i in names(data)) {
if (length(which(isUnknown(x=data[[i]], unknown = c("","#VALUE!")))) > 0) {
data[[i]] <- unknownToNA(x=data[[i]], unknown=c("","#VALUE!"))
print(paste(i,"replaced garbage=", any(isUnknown(x=data[[i]], unknown = c("","#VALUE!"))), sep = " "))
}
}
str(data)
# detect the numerical columns to remove outliers
# data.numericals <- data[, which(names(data) %in% outliers)]
# str(data.numericals)
# for(i in names(data.numericals)) {
#   quantiles <- quantile(data.numericals[[i]], c(.05, .95), na.rm = TRUE)
#   if (quantiles[1] >= 0 && quantiles[2] >= 0) {
#     print(i)
#     data.numericals[[i]][ data.numericals[[i]] < quantiles[1] ] <- quantiles[1]
#     data.numericals[[i]][ data.numericals[[i]] > quantiles[2] ] <- quantiles[2]
#   }
# }
# data.others <- data[ , -which(names(data) %in% outliers)]
# str(data.others)
# data.cleaned <- cbind(data.numericals, data.others)
#
# write.table(data.cleaned, "data_edits1.csv", sep=",", row.names=FALSE, quote = FALSE)
# str(data.cleaned)
data.cleaned <- data
# set 10% missing threshold
cols <- ncol(data.cleaned)
rows <- nrow(data.cleaned)
NA_RowWise <- apply(data.cleaned, 1, function(z) sum(is.na(z)))
hist(NA_RowWise)
quantile(NA_RowWise)
# eliminate rows whose missing > 20%
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
data.rowsCleaned <- data.cleaned
if (length(DelRows) > 0) {
data.rowsCleaned <- data.cleaned[-DelRows,]
}
nrow(data.rowsCleaned)
ncol(data.rowsCleaned)
NA_ColWise <- apply(data.rowsCleaned, 2, function(z) sum(is.na(z)))
hist(NA_ColWise)
quantile(NA_ColWise)
#eliminate cols whose missing > 20%
rowsMissing <- floor(rows*missingThreshold)
rowsMissing
DelCols <- which(NA_ColWise > rowsMissing)
length(DelCols)
data.colsCleaned <- data.rowsCleaned
if (length(DelCols) > 0) {
data.colsCleaned <- data.rowsCleaned[,-DelCols]
}
nrow(data.colsCleaned)
ncol(data.colsCleaned)
write.table(data.colsCleaned, "data_cleaned2.csv", sep=",", row.names=FALSE, quote = FALSE)
# make data point to the cleaned up version
data <- data.colsCleaned
nearZeroVariance <- nearZeroVar(data[,-which(names(data) %in% c(ID,METRIC))]) #, saveMetrics = T
names <- names(data[,-which(names(data) %in% c(ID,METRIC))])[nearZeroVariance]
names
if (length(nearZeroVariance) > 0)
data <- data[,-which(names(data) %in% names)]
str(data)
# Categorical labeling - find out type of columns
types <- lapply(data, class)
distincts <- lapply(data, function(c) unique(c))
for(i in names(data[,-which(names(data) %in% c(ID,METRIC))])){
noOfCats <- length(levels(distincts[[i]]))
if (noOfCats == 1) {
data[[i]] <- NULL
} else if ((noOfCats <= length(data[[i]])/2 && types[[i]] == "factor") || types[[i]] == "character") {
print(paste("processing ====", i, sep = "> "))
means <- sapply(split(data$EUR_o..Mstb., data[[i]]), mean)
print(names(means))
# convert to character type
data[[i]] <- as.character(data[[i]])
for(j in names(means)) {
print(paste("replacing", j, "by", means[[j]], sep = " "))
data[[i]][data[[i]] == j] <- means[[j]]
}
data[[i]] <- as.numeric(data[[i]])
}
}
write.table(data, "data_labeled3.csv", sep=",", row.names=FALSE, quote = FALSE)
str(data)
transformed <- rfImpute(EUR_o..Mstb. ~ ., data = data[,-which(names(data) %in% c(ID))])
fit <- M5P(EUR_o..Mstb. ~. , data = transformed)
s<-summary(fit)
rmse <- s$details[["rootMeanSquaredError"]]
rmse
set.seed(1)
transformed <- rfImpute(EUR_o..Mstb. ~ ., data = data[,-which(names(data) %in% c(ID))])
fit <- M5P(EUR_o..Mstb. ~. , data = transformed)
s<-summary(fit)
rmse <- s$details[["rootMeanSquaredError"]]
rmse
# clear the memory!
rm(list=ls(all=TRUE))
gc()
# set the work directory
setwd("~/Chevron/DSChallenge/optimize/results")
# some tweaking constants
IMP_VARS_CNT <- 15
missingThreshold <- 0.8
transformations <- c("BoxCox", "center", "scale") # ,"bagImpute"
corrCutOff <- 0.80
iterations <- 1000
trainSplit <- 0.70
# imp vars!
ID <- "WellID"
USELESS <- c("Completion.Date")
METRIC <- "EUR_o..Mstb."
outliers <- c("Fluid.Water..Gals.","Acid..Gals.","Gel.x.link..Gals.","Proppant...Total..lbs.","Fluid...Total..lbs.","Fluid.Amount","Propping.Agent.Amount") #"Other..Gals.",
# load base_training.csv
base <- read.csv("../base_training.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# remove the Deepest Zone as it will be added below
base <- base[, -which(names(base) %in% c("Deepest_Zone"))]
# split Between Zone into 2
zones=data.frame(do.call(rbind, strsplit(as.vector(base$Between_Zone), split = " --> ", fixed=TRUE)))
names(zones) <- c("Top_Zone", "Deepest_Zone")
base <- cbind(base, zones)
# format Completion.Date
base$Completion.Date = as.Date(base$Completion.Date, origin = "1900-01-01")
# add Completion.Month
# base$Completion.Month = as.numeric(format(base$Completion.Date, "%m"))
# add Completion.Season
# base$Completion.Season <- time2season(base$Completion.Date, out.fmt = "seasons", type = "default")
write.table(base, "base_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load completions_training.csv with Remarks removed!
completions <- read.csv("../completions_training_removedRemarks.csv", header = T)
# unique completions
uniqueCompletions <- unique( completions )
write.table(uniqueCompletions, "completions_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load geology_training.csv
geo <- read.csv("../geology_training.csv", header = TRUE, sep = ",")
# mark the handpicked columns
handpicked <- c("CLFK..KHW.","U_SPBR..KHW.","L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.","ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
# mark the interpolated columns
interpolated <- c("CLEARFORK..MAP.","SPRABERRY_U..MAP.","SPRABERRY_L..MAP.","WOLFCAMP..MA","WOLFCAMP_L..","WFMP_ATB..MA","CLINE..MAP.","STRAWN..MAP.","ATOKA..MAP.","BEND..MAP.","ATOKA_L..MAP","MISSISSIPPIA","WOODFORD..MA","DEVINOAN_UNC")
# replace the Handpicked NAs by values from the Interpolated columns
for(i in 1:length(handpicked)) {
print(paste(handpicked[i], interpolated[i], sep = " = "))
geo[[handpicked[i]]][is.na(geo[[handpicked[i]]])] <- geo[[interpolated[i]]][is.na(geo[[handpicked[i]]])]
}
# drop interpolated columns
geo = geo[,-which(names(geo) %in% interpolated)]
write.table(geo, "geology_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# merging the files
temp <- merge(base, uniqueCompletions, by="WellID")
data <- merge(temp, geo, by="WellID")
write.table(data, "data.csv", sep=",", row.names=FALSE, quote = FALSE)
print(paste("=============== Data merged into one file with", nrow(data), "rows and", ncol(data), "columns ===============", sep = " "))
# remove some unwanted columns
data <- data[, -which(names(data) %in% USELESS)]
str(data)
# replace empty strings and #VALUE! as NAs for all columns
for(i in names(data)) {
if (length(which(isUnknown(x=data[[i]], unknown = c("","#VALUE!")))) > 0) {
data[[i]] <- unknownToNA(x=data[[i]], unknown=c("","#VALUE!"))
print(paste(i,"replaced garbage=", any(isUnknown(x=data[[i]], unknown = c("","#VALUE!"))), sep = " "))
}
}
str(data)
data.cleaned <- data
# set 10% missing threshold
cols <- ncol(data.cleaned)
rows <- nrow(data.cleaned)
NA_RowWise <- apply(data.cleaned, 1, function(z) sum(is.na(z)))
hist(NA_RowWise)
quantile(NA_RowWise)
# eliminate rows whose missing > 20%
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
data.rowsCleaned <- data.cleaned
if (length(DelRows) > 0) {
data.rowsCleaned <- data.cleaned[-DelRows,]
}
nrow(data.rowsCleaned)
ncol(data.rowsCleaned)
NA_ColWise <- apply(data.rowsCleaned, 2, function(z) sum(is.na(z)))
hist(NA_ColWise)
quantile(NA_ColWise)
#eliminate cols whose missing > 20%
rowsMissing <- floor(rows*missingThreshold)
rowsMissing
DelCols <- which(NA_ColWise > rowsMissing)
length(DelCols)
data.colsCleaned <- data.rowsCleaned
if (length(DelCols) > 0) {
data.colsCleaned <- data.rowsCleaned[,-DelCols]
}
nrow(data.colsCleaned)
ncol(data.colsCleaned)
write.table(data.colsCleaned, "data_cleaned2.csv", sep=",", row.names=FALSE, quote = FALSE)
# make data point to the cleaned up version
data <- data.colsCleaned
# Categorical labeling - find out type of columns
types <- lapply(data, class)
distincts <- lapply(data, function(c) unique(c))
for(i in names(data[,-which(names(data) %in% c(ID,METRIC))])){
noOfCats <- length(levels(distincts[[i]]))
if (noOfCats == 1) {
data[[i]] <- NULL
} else if ((noOfCats <= length(data[[i]])/2 && types[[i]] == "factor") || types[[i]] == "character") {
print(paste("processing ====", i, sep = "> "))
means <- sapply(split(data$EUR_o..Mstb., data[[i]]), mean)
print(names(means))
# convert to character type
data[[i]] <- as.character(data[[i]])
for(j in names(means)) {
print(paste("replacing", j, "by", means[[j]], sep = " "))
data[[i]][data[[i]] == j] <- means[[j]]
}
data[[i]] <- as.numeric(data[[i]])
}
}
write.table(data, "data_labeled3.csv", sep=",", row.names=FALSE, quote = FALSE)
str(data)
# find columns which have near zero variance
nearZeroVariance <- nearZeroVar(data[,-which(names(data) %in% c(ID,METRIC))]) #, saveMetrics = T
names <- names(data[,-which(names(data) %in% c(ID,METRIC))])[nearZeroVariance]
names
if (length(nearZeroVariance) > 0)
data <- data[,-which(names(data) %in% names)]
str(data)
set.seed(1)
transformed <- rfImpute(EUR_o..Mstb. ~ ., data = data[,-which(names(data) %in% c(ID))])
fit <- M5P(EUR_o..Mstb. ~. , data = transformed)
s<-summary(fit)
rmse <- s$details[["rootMeanSquaredError"]]
rmse
set.seed(1)
transformed <- rfImpute(EUR_o..Mstb. ~ ., data = data[,-which(names(data) %in% c(ID))],iter = 100, ntree = 50)
set.seed(1)
transformed <- rfImpute(EUR_o..Mstb. ~ ., data = data[,-which(names(data) %in% c(ID))],iter = 10, ntree = 100)
fit <- M5P(EUR_o..Mstb. ~. , data = transformed)
s<-summary(fit)
rmse <- s$details[["rootMeanSquaredError"]]
rmse
# impute the missing values - lets test this!
optimizer <- function(x) {
seed <- x[1]
iter <- x[2]
ntree <- x[3]
temp <- data[,-which(names(data) %in% c(ID))]
set.seed(seed)
temp <- rfImpute(EUR_o..Mstb. ~., temp,iter = iter,ntree = ntree)
fit <- M5P(EUR_o..Mstb. ~. , data = temp)
s<-summary(fit)
rmse <- s$details[["rootMeanSquaredError"]]
rmse
}
x_init <- c(100,10,50)
optimizer(x_init)
optim <- optim(x_init,optimizer)
# impute the missing values - lets test this!
optimizer <- function(x) {
seed <- x[1]
#   iter <- x[2]
ntree <- x[2]
temp <- data[,-which(names(data) %in% c(ID))]
set.seed(seed)
temp <- rfImpute(EUR_o..Mstb. ~., temp,iter = 5,ntree = ntree)
fit <- M5P(EUR_o..Mstb. ~. , data = temp)
s<-summary(fit)
rmse <- s$details[["rootMeanSquaredError"]]
rmse
}
x_init <- c(100,50)
optimizer(x_init)
optim <- optim(x_init,optimizer,control = list(maxit=100))
optim
x_init <- c(106,52)
optimizer(x_init)
optim$par[1]
floor(optim$par[1])
temp <- data[,-which(names(data) %in% c(ID))]
set.seed(floor(optim$par[1]))
temp <- rfImpute(EUR_o..Mstb. ~., temp,iter = 5,ntree = floor(optim$par[2]))
data <- cbind(data[[ID]], temp)
str(data)
colnames(data)[1] <- ID
str(data)
rf <- randomForest(EUR_o..Mstb. ~., data=data[,-which(names(data) %in% c(ID))], mtry=2, ntree=50, importance=TRUE)
ranks <- importance(rf,type=1)
# sort in descending order
ranks <- sort(ranks[,],decreasing = T)
ranks
rmse <- rmse(rf$y,rf$predicted)
rmse
# running the randomForest implementation for getting importance using optimizer!
temp <- data[,-which(names(data) %in% c(ID))]
M <- ncol(temp)
# optimizer to get ranks
optimizer <- function(x) {
seed <- x[1]
mtry <- x[2]
ntree <- x[3]
set.seed(seed)
# running the randomForest implementation for getting importance
rf <- randomForest(EUR_o..Mstb. ~., data=temp, mtry=mtry, ntree=ntree, importance=TRUE)
rmse <- rmse(rf$y,rf$predicted)
rmse
}
x_init <- c(99,6,47)
optimizer(x_init)
x_init <- c(100,M-10,50)
optimizer(x_init)
x_init <- c(100,M-10,100)
optimizer(x_init)
optim <- optim(x_init,optimizer,control = list(trace=0,maxit=1000))
optim <- optim(x_init,optimizer,control = list(trace=1,maxit=1000))
x_init <- c(100,20,50)
optimizer(x_init)
x_init <- c(100,20,70)
optimizer(x_init)
x_init <- c(100,30,70)
optimizer(x_init)
x_init <- c(100,40,50)
optimizer(x_init)
temp <- data[,-which(names(data) %in% c(ID))]
M <- ncol(temp)
# optimizer to get ranks
optimizer <- function(x) {
seed <- x[1]
mtry <- x[2]
ntree <- x[3]
set.seed(seed)
# running the randomForest implementation for getting importance
rf <- randomForest(EUR_o..Mstb. ~., data=temp, mtry=mtry, ntree=ntree, importance=TRUE)
rmse <- rmse(rf$y,rf$predicted)
rmse
}
x_init <- c(100,40,50)
optimizer(x_init)
optim <- optim(x_init,optimizer,control = list(trace=0,maxit=100))
optim
x_init <- c(106,M-1,52)
optimizer(x_init)
x_init <- c(106,M,52)
optimizer(x_init)
x_init <- c(106,M-2,52)
optimizer(x_init)
x_init <- c(105,35,55)
optimizer(x_init)
optim
set.seed(floor(optim$par[1]))
rf <- randomForest(EUR_o..Mstb. ~., data=temp, mtry=floor(optim$par[2]), ntree=floor(optim$par[3]), importance=TRUE)
# rf <- randomForest(EUR_o..Mstb. ~., data=data[,-which(names(data) %in% c(ID))], mtry=2, ntree=50, importance=TRUE)
rmse <- rmse(rf$y,rf$predicted)
rmse
ranks <- importance(rf,type=1)
# sort in descending order
ranks <- sort(ranks[,],decreasing = T)
ranks
isImportant <- function(X) {
X[ifelse(X >= 0,TRUE,FALSE)]
}
ranks <- names(isImportant(ranks))
ranks
ranks[length(ranks)+1] <- METRIC
ranks[length(ranks)+1] <- ID
data <- data[,which(names(data) %in% ranks)]
str(data)
write.table(data, "data_important.csv", sep=",", row.names=FALSE, quote = FALSE)
forTraining <- data[,-which(names(data) %in% c(ID))]
N <- nrow(forTraining)
# modeling
best.rmse <- 10000
best.train <- NULL
best.test <- NULL
best.fit <- NULL
best.cor <- -1
best.seed <- -1
# n iterations to get best model
for(i in 1:iterations) {
# set seed by iteration
set.seed(i)
# do random sampling to generate training & test sets
trainIndex <- sample(nrow(data),floor(nrow(data)*trainSplit))
# createDataPartition(data$Operator.Numeric, p = .7,list = FALSE,times = 1)
train=data[trainIndex,-which(names(data) %in% c(ID))]
test=data[-trainIndex,-which(names(data) %in% c(ID))]
# the model - M5P from RWeka package
fit <- M5P(EUR_o..Mstb. ~. , data = train, na.action = na.pass)
predicted <- predict(fit, newdata = test)
#Convert predicted response vector as dataframe
actual= test$EUR_o..Mstb.
predicted <- as.data.frame(predicted)
colnames(predicted) <- "predicted.EUR"
#Replacing NA with average EUR from training set (NA not allowed for submission)
# predicted[is.na(predicted)] <- mean(train$EUR_o..Mstb.)
# The RMSE
RMSE<-sqrt((sum((actual-predicted)^2))/nrow(test))
COR <- cor(actual, predicted)
if (RMSE < best.rmse && COR > best.cor) {
print(paste("====>", COR, RMSE, i, sep = " "))
best.rmse <- RMSE
best.train <- train
best.test <- test
best.fit <- fit
best.cor <- COR
best.seed <- i
}
}
# print results of the best iteration
summary(best.fit)
print(best.rmse)
print(best.cor)
print(best.seed)
str(data)
nonactionables <- c("Subarea","Operator","County","Completion.Year","Surface.Latitude","Surface.Longitude")
N
# save the model to disk
save(best.fit, file='completions.model')
# load the model back from disk (prior variable name is restored)
# load('completions.model')
actual <- best.test$CumProd
predicted <- predict(best.fit,best.test)
#
final = cbind(actual,predicted)
write.table(final, "output.csv",sep=",",row.names=FALSE, quote = FALSE)
M <- ncol(data)
names <- names(data)
names[length(names)+1] <- "PredEUR"
names[length(names)+1] <- "PercentInc"
optimums <- data.frame(matrix(vector(), 0, M+2, dimnames=list(c(), names)), stringsAsFactors=T)
actionables <- names(data)[!(names(data) %in% nonactionables)]
actionables <- actionables[!(actionables %in% c(ID))]
actionables
well <- 19
stub <- data[well, ]
stub
# create optimization wrapper for CUM
opt_gm <- function(x, known.x) { #
#     x <- opt_start*1.1
#     known.x <- opt_nons
#     x <- ifelse(is.na(known.x), opt.x, known.x)
z <- stub[,-which(names(stub) %in% c(ID,METRIC))]
# copy the data over top of the stub record
for (i in names(x)) {
#       i <- "X..Clusters"
if (i %in% actionables) {
#         print(paste("actionable",i,x[[i]],sep = " = "))
z[[i]] <- x[[i]]
} else {
#         print(paste("non-actionable",i,known.x[[i]],sep = " = "))
z[[i]] <- known.x[[i]]
}
}
# score the data and return the negative
y <- predict(best.fit,z)
-y
}
start <- stub[,-which(names(stub) %in% c(ID,METRIC))]
opt_start <- start
#   opt_start <- as.numeric(as.vector(opt_start[1,]))
for (i in names(opt_start)) {
if (i %in% actionables)
opt_start[[i]] <- opt_start[[i]]*1.1
}
opt_start
# lower
opt_min <- start
for (i in names(opt_min)) {
#     if (i %in% actionables)
opt_min[[i]] <- min(data[[i]],na.rm = T)
}
opt_min
# upper
opt_max <- start
for (i in names(opt_max)) {
#     if (i %in% actionables)
opt_max[[i]] <- max(data[[i]],na.rm = T)
}
opt_max
opt_nons <- start
for (i in names(opt_nons)) {
if (i %in% actionables)
opt_nons[[i]] <- NA
}
opt_nons
opt_results <- optim(opt_start, opt_gm, method="L-BFGS-B", control = list(trace=0,maxit=1000,REPORT=1),lower=opt_min, upper=opt_max,known.x=opt_nons) # ,
# view the optimized inputs & predicted output (CUM)
opt_results
opt_results <- nlminb(opt_start, opt_gm, scale = 1, control = list(iter.max=1000,trace=1), lower = opt_min, upper = opt_max,known.x=opt_nons)
opt_results
temp <- stub
temp$PredCum <- stub$CumProd
for(i in names(opt_results$par)) {
temp[[i]] <-  opt_results$par[[i]]
}
temp$CumProd <- abs(opt_results$value)
temp$PctInc <- (abs(opt_results$value) - temp$PredCum)/temp$PredCum
#     temp$CumProd <- abs(opt_results$objective)
#     temp$PctInc <- (abs(opt_results$objective) - temp$PredCum)/temp$PredCum
temp$PctInc <- temp$PctInc * 100
temp <- stub
temp$PredEUR <- stub$EUR_o..Mstb.
for(i in names(opt_results$par)) {
temp[[i]] <-  opt_results$par[[i]]
}
temp$EUR_o..Mstb. <- abs(opt_results$value)
temp$PercentInc <- (abs(opt_results$value) - temp$PredEUR)/temp$PredEUR
#     temp$CumProd <- abs(opt_results$objective)
#     temp$PctInc <- (abs(opt_results$objective) - temp$PredCum)/temp$PredCum
temp$PercentInc <- temp$PercentInc * 100
temp
