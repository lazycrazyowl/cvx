if (is.null(temp[[i]]))
next
print(paste("processing =========",i,sep = " "))
highest.cor <- -1
alias <- NULL
t <- temp[,-which(names(temp) %in% c(i))]
for(j in names(t)) {
if (is.null(t[[j]]))
next
cor <- cor.test(temp[[i]],t[[j]],na.action = "na.exclude")
if (!is.na(cor$estimate) && cor$estimate > highest.cor) {
highest.cor <- cor$estimate
alias <- j
}
}
if (highest.cor > naCutOff) {
iNAs <- length(which(is.na(temp[[i]])))
jNAs <- length(which(is.na(temp[[alias]])))
print(paste(i,iNAs,alias,jNAs,sep = " "))
if (iNAs <= jNAs) {
if (F && any(is.na(temp[[i]]))) {
print(paste("-------------------------------- interpolating",i,highest.cor,sep = " "))
interpolate <- approxfun(temp[[alias]], temp[[i]],rule = 2)
predicted <- interpolate(temp[[alias]])
actual= temp[[i]]
RMSE<-RMSE(pred = predicted,obs = actual,na.rm = T)
RMSE
actual[is.na(actual)] <- predicted[is.na(actual)]
temp[[i]] <- actual
}
if (highest.cor > corrCutOff) {
print(paste("-------------------------------- dropping",alias,highest.cor,sep = " "))
temp <- temp[,-which(names(temp) %in% c(alias))]
}
} else if (iNAs > jNAs) {
if (F && any(is.na(temp[[alias]]))) {
print(paste("-------------------------------- interpolating",alias,highest.cor,sep = " "))
interpolate <- approxfun(temp[[i]], temp[[alias]],rule = 2)
predicted <- interpolate(temp[[i]])
actual= temp[[alias]]
RMSE<-RMSE(pred = predicted,obs = actual,na.rm = T)
RMSE
actual[is.na(actual)] <- predicted[is.na(actual)]
temp[[alias]] <- actual
}
if (highest.cor > corrCutOff) {
print(paste("-------------------------------- dropping",i,highest.cor,sep = " "))
temp <- temp[,-which(names(temp) %in% c(i))]
}
}
}
}
print("Impute the missing values from knnImpute!")
i <- impKNNa(x = temp, method = "knn", k = 3, metric = "Euclidean")
temp <- as.data.frame(i$xImp)
# trans <- preProcess(imp, method =  c("knnImpute"))
# imp <- predict(object = trans, newdata = imp)
temp <- cbind(data[[METRIC]], temp)
colnames(temp)[1] <- METRIC
data <- cbind(data[[ID]], temp)
colnames(data)[1] <- ID
str(data)
# let's set aside some "unseen" stratified data from the model!
set.seed(100)
indx <- createDataPartition(data$EUR_o..Mstb.,p = trainSplit)
index <- indx$Resample1
testing=data[-index,]
data=data[index,]
write.table(data, "train.csv",sep=",",row.names=FALSE, quote = FALSE)
write.table(testing, "test.csv",sep=",",row.names=FALSE, quote = FALSE)
# TODO change the cut-offs
# LOW <- 0.98
# marks <- as.numeric(quantile(training$EUR_o..Mstb., c(0, LOW, 1)))
# marks
# training$Class <- cut(x = training$EUR_o..Mstb., breaks = marks, include.lowest = T, labels = F)
# table(training$Class)
# the normal fit that we do!
result <- challengeRegression(data)
# clear the memory!
rm(list=ls(all=TRUE))
gc()
# load the dependencies!
require(gdata)
require(randomForest)
require(RWeka)
require(caret)
require(fmsb)
library(foreach)
library(Metrics)
library(e1071)
library(corrplot)
library(RANN)
library(ipred)
library(plyr)
library(rJava)
library(dfoptim)
library(proxy)
library(googleVis)
library(DMwR)
library(rpart)
library(earth)
library(kernlab)
library(MASS)
library(elasticnet)
library(robustbase)
library(FSelector)
library(robCompositions)
# set the work directory
setwd("~/Chevron/DSChallenge/using-optimizations/results")
if (!exists('challengeRegression', mode = "function"))
source("../challenge-regression.R")
if (!exists('challengeOptimizations', mode = "function"))
source("../challenge-optimizations.R")
# load base_training.csv
base <- read.csv("../base_training.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# remove the Deepest Zone as it will be added below
base <- base[, -which(names(base) %in% c("Deepest_Zone"))]
# split Between Zone into 2
zones=data.frame(do.call(rbind, strsplit(as.vector(base$Between_Zone), split = " --> ", fixed=TRUE)))
names(zones) <- c("Top_Zone", "Deepest_Zone")
base <- cbind(base, zones)
write.table(base, "base_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load completions_training.csv with Remarks removed!
completions <- read.csv("../completions_training_removedRemarks.csv", header = T)
# unique completions
uniqueCompletions <- unique( completions )
uniqueCompletions$Treated.Stage.Height <- uniqueCompletions$Depth.Base - uniqueCompletions$Depth.Top
uniqueCompletions$Fluid.Amount.Per.Foot <- uniqueCompletions$Fluid.Amount / uniqueCompletions$Treated.Stage.Height
uniqueCompletions$Propping.Agent.Amount.Per.Foot <- uniqueCompletions$Propping.Agent.Amount / uniqueCompletions$Treated.Stage.Height
write.table(uniqueCompletions, "completions_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load geology_training.csv
geo <- read.csv("../geology_training.csv", header = TRUE, sep = ",")
# mark the handpicked columns
handpicked <- c("CLFK..KHW.","U_SPBR..KHW.","L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.","ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
# mark the interpolated columns
interpolated <- c("CLEARFORK..MAP.","SPRABERRY_U..MAP.","SPRABERRY_L..MAP.","WOLFCAMP..MA","WOLFCAMP_L..","WFMP_ATB..MA","CLINE..MAP.","STRAWN..MAP.","ATOKA..MAP.","BEND..MAP.","ATOKA_L..MAP","MISSISSIPPIA","WOODFORD..MA","DEVINOAN_UNC")
# replace the Handpicked NAs by values from the Interpolated columns
for(i in 1:length(handpicked)) {
print(paste(handpicked[i], interpolated[i], sep = " = "))
geo[[handpicked[i]]][is.na(geo[[handpicked[i]]])] <- geo[[interpolated[i]]][is.na(geo[[handpicked[i]]])]
}
# drop interpolated columns
geo = geo[,-which(names(geo) %in% interpolated)]
colnames(geo) <- c("WellID","Clearfork","Upper_Spraberry","Lower_Spraberry","Wolfcamp","Lower_Wolfcamp","Amacker_Tippet_Base","Cline",
"Strawn","Atoka","Bend","Lower_Atoka","Mississippian","Woodford","Devonian")
write.table(geo, "geology_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load scores derived from Remarks!
# scores <- read.csv("../score.csv", header = T)
# merging the files
temp <- merge(base, uniqueCompletions, by="WellID")
DATA <- merge(temp, geo, by="WellID")
# DATA <- merge(DATA, scores, by="WellID")
# replace geo names in top & between zones
DATA$Top_Zone <- as.character(DATA$Top_Zone)
DATA$Top_Zone[DATA$Top_Zone %in% c("CLFK")] <- "Clearfork"
DATA$Top_Zone[DATA$Top_Zone %in% c("SPBR_L")] <- "Lower_Spraberry"
DATA$Top_Zone[DATA$Top_Zone %in% c("SPBR_U")] <- "Upper_Spraberry"
DATA$Top_Zone[DATA$Top_Zone %in% c("STRAWN")] <- "Strawn"
DATA$Top_Zone[DATA$Top_Zone %in% c("WFMP")] <- "Wolfcamp"
DATA$Deepest_Zone <- as.character(DATA$Deepest_Zone)
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("ATOKA")] <- "Atoka"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("MISS")] <- "Mississippian"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("SPBR_L")] <- "Lower_Spraberry"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("STRAWN")] <- "Strawn"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("WFMP")] <- "Wolfcamp"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("WOOD")] <- "Woodford"
DATA[["Total.Height"]] <- 0
for(i in 1:nrow(DATA)) {
if (!is.null(DATA[[DATA$Deepest_Zone[i]]]) & !is.null(DATA[[DATA$Top_Zone[i]]])) #
DATA[["Total.Height"]][i] <- DATA[[DATA$Deepest_Zone[i]]][i] - DATA[[DATA$Top_Zone[i]]][i]
}
DATA$Total.Height<- unknownToNA(x=DATA$Total.Height, unknown=c(0))
write.table(DATA, "data.csv", sep=",", row.names=FALSE, quote = FALSE)
print(paste("=============== Data merged into one file with", nrow(DATA), "rows and", ncol(DATA), "columns ===============", sep = " "))
str(DATA)
# some tweaking constants
missingThreshold <- 0.7
# transformations <- c("BoxCox", "center", "scale") # ,"bagImpute"
iterations <- 100
VIF_THRESHOLD <- 10
IMP_VARS_CNT <- 10
trainSplit <- 0.70
corrCutOff <- 0.90
naCutOff <- 0.70
# imp vars!
ID <- "WellID"
USELESS <- c("Depth.Top","Propping.Agent.Amount","Propping.Agent.Units","Fluid.Units") #,"Propping.Agent.Units","Between_Zone","Fluid.Units"
METRIC <- "EUR_o..Mstb."
outliers <- c("Fluid.Water..Gals.","Acid..Gals.","Gel.x.link..Gals.","Proppant...Total..lbs.","Fluid...Total..lbs.","Fluid.Amount","Propping.Agent.Amount","Propping.Agent.Amount.Per.Foot","Fluid.Amount.Per.Foot") #"Other..Gals.",
nonactionables <- c("Subarea","Operator","County","Completion.Date","Completion.Year","Surface.Latitude","Surface.Longitude","Depth.Total.Driller..ft.","Between_Zone",
"Top_Zone","Deepest_Zone","Depth.Top","Depth.Base","Fluid.Units","Propping.Agent.Units","CLFK..KHW.","U_SPBR..KHW.",
"L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.",
"ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
DATA <- DATA[, -which(names(DATA) %in% USELESS)]
# replace empty strings and #VALUE! as NAs for all columns
for(i in names(DATA)) {
if (length(which(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!")))) > 0) {
DATA[[i]] <- unknownToNA(x=DATA[[i]], unknown=c("","#VALUE!"))
print(paste(i,"replaced garbage=", any(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!"))), sep = " "))
}
}
# str(DATA)
# detect the numerical columns to remove outliers
data.numericals <- DATA[, which(names(DATA) %in% outliers)]
str(data.numericals)
for(i in names(data.numericals)) {
quantiles <- quantile(data.numericals[[i]], c(.01, .99), na.rm = TRUE)
if (quantiles[1] >= 0 && quantiles[2] >= 0) {
print(i)
data.numericals[[i]][ data.numericals[[i]] < quantiles[1] ] <- quantiles[1]
data.numericals[[i]][ data.numericals[[i]] > quantiles[2] ] <- quantiles[2]
}
}
data.others <- DATA[ , -which(names(DATA) %in% outliers)]
str(data.others)
data.cleaned <- cbind(data.numericals, data.others)
write.table(data.cleaned, "data_edits1.csv", sep=",", row.names=FALSE, quote = FALSE)
str(data.cleaned)
cols <- ncol(data.cleaned)
rows <- nrow(data.cleaned)
NA_RowWise <- apply(data.cleaned, 1, function(z) sum(is.na(z)))
NA_RowWise
min(NA_RowWise)
max(NA_RowWise)
hist(NA_RowWise)
quantile(NA_RowWise)
mean(NA_RowWise)
colsMissing <- floor(cols*missingThreshold)
colsMissing
missingThreshold
cols
missingThreshold <- 0.5
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
missingThreshold <- 0.4
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
DelRows
data.cleaned[DelRows,]
View(data.cleaned[DelRows,])
missingThreshold
missingThreshold <- 0.35
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
missingThreshold <- 0.3
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
View(data.cleaned[DelRows,])
missingThreshold <- 0.4
# transformations <- c("BoxCox", "center", "scale") # ,"bagImpute"
iterations <- 100
VIF_THRESHOLD <- 10
IMP_VARS_CNT <- 10
trainSplit <- 0.70
corrCutOff <- 0.90
naCutOff <- 0.70
# imp vars!
ID <- "WellID"
USELESS <- c("Depth.Top","Propping.Agent.Amount","Propping.Agent.Units","Fluid.Units") #,"Propping.Agent.Units","Between_Zone","Fluid.Units"
METRIC <- "EUR_o..Mstb."
outliers <- c("Fluid.Water..Gals.","Acid..Gals.","Gel.x.link..Gals.","Proppant...Total..lbs.","Fluid...Total..lbs.","Fluid.Amount","Propping.Agent.Amount","Propping.Agent.Amount.Per.Foot","Fluid.Amount.Per.Foot") #"Other..Gals.",
nonactionables <- c("Subarea","Operator","County","Completion.Date","Completion.Year","Surface.Latitude","Surface.Longitude","Depth.Total.Driller..ft.","Between_Zone",
"Top_Zone","Deepest_Zone","Depth.Top","Depth.Base","Fluid.Units","Propping.Agent.Units","CLFK..KHW.","U_SPBR..KHW.",
"L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.",
"ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
DATA <- DATA[, -which(names(DATA) %in% USELESS)]
# replace empty strings and #VALUE! as NAs for all columns
for(i in names(DATA)) {
if (length(which(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!")))) > 0) {
DATA[[i]] <- unknownToNA(x=DATA[[i]], unknown=c("","#VALUE!"))
print(paste(i,"replaced garbage=", any(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!"))), sep = " "))
}
}
# str(DATA)
# detect the numerical columns to remove outliers
data.numericals <- DATA[, which(names(DATA) %in% outliers)]
str(data.numericals)
for(i in names(data.numericals)) {
quantiles <- quantile(data.numericals[[i]], c(.01, .99), na.rm = TRUE)
if (quantiles[1] >= 0 && quantiles[2] >= 0) {
print(i)
data.numericals[[i]][ data.numericals[[i]] < quantiles[1] ] <- quantiles[1]
data.numericals[[i]][ data.numericals[[i]] > quantiles[2] ] <- quantiles[2]
}
}
data.others <- DATA[ , -which(names(DATA) %in% outliers)]
str(data.others)
data.cleaned <- cbind(data.numericals, data.others)
write.table(data.cleaned, "data_edits1.csv", sep=",", row.names=FALSE, quote = FALSE)
str(data.cleaned)
# data.cleaned <- DATA
# set 10% missing threshold
cols <- ncol(data.cleaned)
rows <- nrow(data.cleaned)
NA_RowWise <- apply(data.cleaned, 1, function(z) sum(is.na(z)))
hist(NA_RowWise)
quantile(NA_RowWise)
# eliminate rows whose missing > 20%
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
View(data.cleaned[DelRows,])
# clear the memory!
rm(list=ls(all=TRUE))
gc()
# load the dependencies!
require(gdata)
require(randomForest)
require(RWeka)
require(caret)
require(fmsb)
library(foreach)
library(Metrics)
library(e1071)
library(corrplot)
library(RANN)
library(ipred)
library(plyr)
library(rJava)
library(dfoptim)
library(proxy)
library(googleVis)
library(DMwR)
library(rpart)
library(earth)
library(kernlab)
library(MASS)
library(elasticnet)
library(robustbase)
library(FSelector)
library(robCompositions)
# set the work directory
setwd("~/Chevron/DSChallenge/using-optimizations/results")
if (!exists('challengeRegression', mode = "function"))
source("../challenge-regression.R")
if (!exists('challengeOptimizations', mode = "function"))
source("../challenge-optimizations.R")
# load base_training.csv
base <- read.csv("../base_training.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# remove the Deepest Zone as it will be added below
base <- base[, -which(names(base) %in% c("Deepest_Zone"))]
# split Between Zone into 2
zones=data.frame(do.call(rbind, strsplit(as.vector(base$Between_Zone), split = " --> ", fixed=TRUE)))
names(zones) <- c("Top_Zone", "Deepest_Zone")
base <- cbind(base, zones)
write.table(base, "base_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load completions_training.csv with Remarks removed!
completions <- read.csv("../completions_training_removedRemarks.csv", header = T)
# unique completions
uniqueCompletions <- unique( completions )
uniqueCompletions$Treated.Stage.Height <- uniqueCompletions$Depth.Base - uniqueCompletions$Depth.Top
uniqueCompletions$Fluid.Amount.Per.Foot <- uniqueCompletions$Fluid.Amount / uniqueCompletions$Treated.Stage.Height
uniqueCompletions$Propping.Agent.Amount.Per.Foot <- uniqueCompletions$Propping.Agent.Amount / uniqueCompletions$Treated.Stage.Height
write.table(uniqueCompletions, "completions_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load geology_training.csv
geo <- read.csv("../geology_training.csv", header = TRUE, sep = ",")
# mark the handpicked columns
handpicked <- c("CLFK..KHW.","U_SPBR..KHW.","L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.","ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
# mark the interpolated columns
interpolated <- c("CLEARFORK..MAP.","SPRABERRY_U..MAP.","SPRABERRY_L..MAP.","WOLFCAMP..MA","WOLFCAMP_L..","WFMP_ATB..MA","CLINE..MAP.","STRAWN..MAP.","ATOKA..MAP.","BEND..MAP.","ATOKA_L..MAP","MISSISSIPPIA","WOODFORD..MA","DEVINOAN_UNC")
# replace the Handpicked NAs by values from the Interpolated columns
for(i in 1:length(handpicked)) {
print(paste(handpicked[i], interpolated[i], sep = " = "))
geo[[handpicked[i]]][is.na(geo[[handpicked[i]]])] <- geo[[interpolated[i]]][is.na(geo[[handpicked[i]]])]
}
# drop interpolated columns
geo = geo[,-which(names(geo) %in% interpolated)]
colnames(geo) <- c("WellID","Clearfork","Upper_Spraberry","Lower_Spraberry","Wolfcamp","Lower_Wolfcamp","Amacker_Tippet_Base","Cline",
"Strawn","Atoka","Bend","Lower_Atoka","Mississippian","Woodford","Devonian")
write.table(geo, "geology_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load scores derived from Remarks!
# scores <- read.csv("../score.csv", header = T)
# merging the files
temp <- merge(base, uniqueCompletions, by="WellID")
DATA <- merge(temp, geo, by="WellID")
# DATA <- merge(DATA, scores, by="WellID")
# replace geo names in top & between zones
DATA$Top_Zone <- as.character(DATA$Top_Zone)
DATA$Top_Zone[DATA$Top_Zone %in% c("CLFK")] <- "Clearfork"
DATA$Top_Zone[DATA$Top_Zone %in% c("SPBR_L")] <- "Lower_Spraberry"
DATA$Top_Zone[DATA$Top_Zone %in% c("SPBR_U")] <- "Upper_Spraberry"
DATA$Top_Zone[DATA$Top_Zone %in% c("STRAWN")] <- "Strawn"
DATA$Top_Zone[DATA$Top_Zone %in% c("WFMP")] <- "Wolfcamp"
DATA$Deepest_Zone <- as.character(DATA$Deepest_Zone)
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("ATOKA")] <- "Atoka"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("MISS")] <- "Mississippian"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("SPBR_L")] <- "Lower_Spraberry"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("STRAWN")] <- "Strawn"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("WFMP")] <- "Wolfcamp"
DATA$Deepest_Zone[DATA$Deepest_Zone %in% c("WOOD")] <- "Woodford"
DATA[["Total.Height"]] <- 0
for(i in 1:nrow(DATA)) {
if (!is.null(DATA[[DATA$Deepest_Zone[i]]]) & !is.null(DATA[[DATA$Top_Zone[i]]])) #
DATA[["Total.Height"]][i] <- DATA[[DATA$Deepest_Zone[i]]][i] - DATA[[DATA$Top_Zone[i]]][i]
}
DATA$Total.Height<- unknownToNA(x=DATA$Total.Height, unknown=c(0))
write.table(DATA, "data.csv", sep=",", row.names=FALSE, quote = FALSE)
print(paste("=============== Data merged into one file with", nrow(DATA), "rows and", ncol(DATA), "columns ===============", sep = " "))
str(DATA)
# some tweaking constants
missingThreshold <- 0.4
# transformations <- c("BoxCox", "center", "scale") # ,"bagImpute"
iterations <- 100
VIF_THRESHOLD <- 10
IMP_VARS_CNT <- 10
trainSplit <- 0.70
corrCutOff <- 0.90
naCutOff <- 0.70
# imp vars!
ID <- "WellID"
USELESS <- c("Depth.Top","Propping.Agent.Amount","Propping.Agent.Units","Fluid.Units") #,"Propping.Agent.Units","Between_Zone","Fluid.Units"
METRIC <- "EUR_o..Mstb."
outliers <- c("Fluid.Water..Gals.","Acid..Gals.","Gel.x.link..Gals.","Proppant...Total..lbs.","Fluid...Total..lbs.","Fluid.Amount","Propping.Agent.Amount","Propping.Agent.Amount.Per.Foot","Fluid.Amount.Per.Foot") #"Other..Gals.",
nonactionables <- c("Subarea","Operator","County","Completion.Date","Completion.Year","Surface.Latitude","Surface.Longitude","Depth.Total.Driller..ft.","Between_Zone",
"Top_Zone","Deepest_Zone","Depth.Top","Depth.Base","Fluid.Units","Propping.Agent.Units","CLFK..KHW.","U_SPBR..KHW.",
"L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.",
"ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
DATA <- DATA[, -which(names(DATA) %in% USELESS)]
# replace empty strings and #VALUE! as NAs for all columns
for(i in names(DATA)) {
if (length(which(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!")))) > 0) {
DATA[[i]] <- unknownToNA(x=DATA[[i]], unknown=c("","#VALUE!"))
print(paste(i,"replaced garbage=", any(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!"))), sep = " "))
}
}
# str(DATA)
# detect the numerical columns to remove outliers
data.numericals <- DATA[, which(names(DATA) %in% outliers)]
str(data.numericals)
for(i in names(data.numericals)) {
quantiles <- quantile(data.numericals[[i]], c(.01, .99), na.rm = TRUE)
if (quantiles[1] >= 0 && quantiles[2] >= 0) {
print(i)
data.numericals[[i]][ data.numericals[[i]] < quantiles[1] ] <- quantiles[1]
data.numericals[[i]][ data.numericals[[i]] > quantiles[2] ] <- quantiles[2]
}
}
data.others <- DATA[ , -which(names(DATA) %in% outliers)]
str(data.others)
data.cleaned <- cbind(data.numericals, data.others)
write.table(data.cleaned, "data_edits1.csv", sep=",", row.names=FALSE, quote = FALSE)
str(data.cleaned)
# data.cleaned <- DATA
# set 10% missing threshold
cols <- ncol(data.cleaned)
rows <- nrow(data.cleaned)
NA_RowWise <- apply(data.cleaned, 1, function(z) sum(is.na(z)))
hist(NA_RowWise)
quantile(NA_RowWise)
# eliminate rows whose missing > 20%
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
View(data.cleaned[DelRows,])
data.rowsCleaned <- data.cleaned
if (length(DelRows) > 0) {
data.rowsCleaned <- data.cleaned[-DelRows,]
}
nrow(data.rowsCleaned)
ncol(data.rowsCleaned)
NA_ColWise <- apply(data.rowsCleaned, 2, function(z) sum(is.na(z)))
hist(NA_ColWise)
quantile(NA_ColWise)
max(NA_ColWise)
mean(NA_ColWise)
rowsMissing <- floor(rows*missingThreshold)
rowsMissing
DelCols <- which(NA_ColWise > rowsMissing)
length(DelCols)
View(data.cleaned[,DelCols])
data.colsCleaned <- data.rowsCleaned
if (length(DelCols) > 0) {
data.colsCleaned <- data.rowsCleaned[,-DelCols]
}
nrow(data.colsCleaned)
ncol(data.colsCleaned)
write.table(data.colsCleaned, "data_cleaned2.csv", sep=",", row.names=FALSE, quote = FALSE)
# make data point to the cleaned up version
data <- data.colsCleaned
data$Mesh.Size <- as.character(data$Mesh.Size)
data$Mesh.Size[data$Mesh.Size %in% c("20/04","20 /40","20/20")] <- "20/40"
data$Mesh.Size[data$Mesh.Size %in% c("20/40 & 16/3")] <- "MIXED"
# category labeling
types <- lapply(data, class)
distincts <- lapply(data, function(c) unique(c))
for(i in names(data)){
noOfCats <- length(levels(distincts[[i]]))
if (noOfCats == 1) {
data[[i]] <- NULL
} else if ((noOfCats <= length(data[[i]])/2 && types[[i]] == "factor") || types[[i]] == "character") {
print(paste("processing ====", i, sep = "> "))
means <- sapply(split(data$EUR_o..Mstb., data[[i]]), function(x) mean(x, na.rm=TRUE))
print(names(means))
# convert to character type
data[[i]] <- as.character(data[[i]])
for(j in names(means)) {
print(paste("replacing", j, "by", means[[j]], sep = " "))
data[[i]][data[[i]] == j] <- means[[j]]
}
data[[i]] <- as.numeric(data[[i]])
}
}
