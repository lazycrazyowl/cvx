data <- medCUMs
model <- medCUMsModel
M <- ncol(data)
N <- nrow(data)
names <- names(data)
actionables <- names(data)[!(names(data) %in% nonactionables)]
actions <- data.frame(matrix(vector(), 0, M, dimnames=list(c(), names)), stringsAsFactors=T)
optimums <- data.frame(matrix(vector(), 0, M, dimnames=list(c(), names)), stringsAsFactors=T)
actionables <- actionables[!(actionables %in% c(ID))]
data$Wellname <- as.character(data$Wellname)
for (well in 1:N) {
#   well <- 7
stub <- data[well, ]
print(stub$Wellname)
# this is to get min-max of Steen Scruggs & Kerner Carson wells
# name=unlist(strsplit(gsub("-", " ", stub$Wellname),split=" ",fixed=TRUE))[1]
# wellsData <- data[grepl(name,data$Wellname),]
# create optimization wrapper for CUM
opt_gm <- function(x, known.x) { #
val <- stub[[METRIC]]
z <- stub[,-which(names(stub) %in% c(ID,METRIC))]
# copy the data over top of the stub record
for (i in names(x)) {
if (i %in% actionables) {
z[[i]] <- x[[i]]
} else {
z[[i]] <- known.x[[i]]
}
}
# score the data and return the negative
y <- predict(model,newdata = z)
#If these conditions are violated, the function returns a large positive number
# which the search procedure will avoid
#     if(y > val*1.1) return(10^38)
-y
}
# start with given values and replace by mean for actionables
start <- stub[,-which(names(stub) %in% c(ID,METRIC))]
# lower
opt_min <- start
for (i in names(opt_min)) {
opt_min[[i]] <- min(data[[i]],na.rm = T)
}
opt_min
# upper
opt_max <- start
for (i in names(opt_max)) {
opt_max[[i]] <- max(data[[i]],na.rm = T)
}
opt_max
opt_start <- start
for (i in names(opt_start)) {
if (i %in% actionables) {
opt_start[[i]] <- mean(data[[i]],na.rm = T)
}
}
opt_start
opt_nons <- start
for (i in names(opt_nons)) {
if (i %in% actionables)
opt_nons[[i]] <- NA
}
opt_nons
opt_gm(opt_start,opt_nons)
# optimize
opt_results <- optim(opt_start, opt_gm, method="L-BFGS-B", control = list(trace=0,maxit=1000,REPORT=1),lower=opt_min, upper=opt_max,known.x=opt_nons) # ,
#   opt_results <- nmkb(opt_start, opt_gm, lower=opt_min, upper=opt_max,known.x=opt_nons) # result ~
# view the optimized inputs & predicted output (CUM)
opt_results
# test the optima
temp <- stub
action <- stub
for(i in names(opt_results$par)) {
opti <- opt_results$par[[i]]
ori <- temp[[i]]
action[[i]] <- "NA"
if (ori != 0)
action[[i]] <- ((opti - ori)/ori)*100
temp[[i]] <-  opti
}
temp$CumProd <- abs(opt_results$value)
action$CumProd <- ((temp$CumProd - stub$CumProd)/stub$CumProd) * 100
optimums[well, ] <- temp
actions[well, ] <- action
}
write.table(optimums, "prescriptions.csv",sep=",",row.names=FALSE, quote = FALSE)
write.table(actions, "actions.csv",sep=",",row.names=FALSE, quote = FALSE)
# clear the memory!
rm(list=ls(all=TRUE))
gc()
# load the dependencies!
require(gdata)
require(randomForest)
require(RWeka)
require(caret)
require(fmsb)
library(foreach)
library(Metrics)
library(e1071)
library(corrplot)
library(RANN)
library(ipred)
library(plyr)
library(rJava)
library(dfoptim)
library(proxy)
library(googleVis)
library(DMwR)
library(rpart)
library(kernlab)
# set the work directory
setwd("/home/bolaka/EOG-data/AllWells")
if (!exists('buildRegression', mode = "function"))
source("buildRegression.R")
# load well wise Completions data
DATA <- read.csv("eogcompletions.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
print(paste("=============== Data has", nrow(DATA), "rows and", ncol(DATA), "columns ===============", sep = " "))
str(DATA)
# some tweaking constants
VIF_THRESHOLD <- 10
trainSplit <- 0.70
corrCutOff <- 0.85
# imp vars!
ID <- "Wellname"
USELESS <- c("CUM_60PD_OIL_PROD","CUM_30PD_OIL_PROD","CUM_90PD_OIL_PROD") #"Avg.ShutIn.Days","ShutinsPeriods","ShutinDays"
METRIC <- "CumProd"
nonactionables <- c("INITIAL_FRAC_GRADIENT","FINAL_FRAC_GRADIENT","SURFACE_LOC_LAT","SURFACE_LOC_LONG","AVG_AZIMUTH","TEAM","TOTAL_VERTICAL_DEPTH")
names(DATA)[names(DATA) == "CUM_180PD_OIL_PROD"] <- METRIC
# replace "undefined", empty strings and #VALUE! as NAs for all columns
for(i in names(DATA)) {
if (length(which(isUnknown(x=DATA[[i]], unknown = c("undefined","","#VALUE!")))) > 0) {
DATA[[i]] <- unknownToNA(x=DATA[[i]], unknown=c("undefined","","#VALUE!"))
print(paste(i,"replaced UNDEFINEDs=", any(isUnknown(x=DATA[[i]], unknown = c("undefined","","#VALUE!"))), sep = " "))
}
}
# str(DATA)
# remove some unwanted columns
original <- DATA
DATA <- DATA[, -which(names(DATA) %in% USELESS)]
# str(DATA)
# hist(x = DATA$CumProd, plot = T)
# divide into training and test sets
testing <- subset(DATA, is.na(DATA$CumProd))
data <- subset(DATA, !is.na(DATA$CumProd))
# category labeling
types <- lapply(data, class)
distincts <- lapply(data, function(c) unique(c))
for(i in names(data)){
noOfCats <- length(levels(distincts[[i]]))
if (noOfCats == 1) {
data[[i]] <- NULL
} else if ((noOfCats <= length(data[[i]])/2 && types[[i]] == "factor") || types[[i]] == "character") {
print(paste("processing ====", i, sep = "> "))
means <- sapply(split(data$CumProd, data[[i]]), function(x) mean(x, na.rm=TRUE))
print(names(means))
# convert to character type
data[[i]] <- as.character(data[[i]])
for(j in names(means)) {
print(paste("replacing", j, "by", means[[j]], sep = " "))
data[[i]][data[[i]] == j] <- means[[j]]
}
data[[i]] <- as.numeric(data[[i]])
}
}
# resultWhole <- buildRegression(data)
# summary(resultWhole$model)
# create different brackets of CUM values!
# LOWs
# lows <- quantile(data$CumProd, c(0, 0.2))
# lowCUMs <- data[data$CumProd>lows[[1]] & data$CumProd<lows[[2]], ]
# str(lowCUMs)
# hist(x = lowCUMs$CumProd, plot = T)
# lowsResult <- buildRegression(lowCUMs)
# lowCUMsModel <- lowsResult$model
# lowCUMs <- lowsResult$data
# summary(lowCUMsModel)
# MEDIUMs
meds <- quantile(data$CumProd, c(0.3, 0.5))
medCUMs <- data[data$CumProd>meds[[1]] & data$CumProd<meds[[2]], ]
str(medCUMs)
hist(x = medCUMs$CumProd, plot = T)
medsResult <- buildRegression(medCUMs)
medCUMsModel <- medsResult$model
medCUMs <- medsResult$data
summary(medCUMsModel)
# medCUMs <- merge(medCUMs, original[,c(1,18:20)], by=ID)
# HIGHs
# highs <- quantile(data$CumProd, c(0.6, 0.8))
# highCUMs <- data[data$CumProd>highs[[1]] & data$CumProd<highs[[2]], ]
# str(highCUMs)
# hist(x = highCUMs$CumProd, plot = T)
# highsResult <- buildRegression(highCUMs)
# highCUMsModel <- highsResult$model
# highCUMs <- highsResult$data
# summary(highCUMsModel)
# VERY HIGHs
# veryhighs <- quantile(data$CumProd, c(0.9, 1.0))
# veryhighCUMs <- data[data$CumProd>veryhighs[[1]] & data$CumProd<veryhighs[[2]], ]
# str(veryhighCUMs)
# hist(x = veryhighCUMs$CumProd, plot = T)
# veryhighsResult <- buildRegression(veryhighCUMs)
# veryhighCUMsModel <- veryhighsResult$model
# veryhighCUMs <- veryhighsResult$data
# summary(veryhighCUMsModel)
# # train on whole data
# fit <- M5P(CumProd ~. , data = forTraining)
# predicted <- predict(fit, newdata = forTraining)
# actual <- forTraining$CumProd
# RMSE <- RMSE(pred = predicted, obs = actual)
# COR <- cor(actual, predicted)
# summary(fit)
# print(RMSE)
# print(COR)
#
# # save the model to disk
# rJava::.jcache(best.fit$classifier)
save(medCUMsModel, file = "medium_completions_gaussian.rda")
# optimization of wells!
write.table(medCUMs, "the_meds.csv", sep=",", row.names=FALSE, quote = FALSE)
data <- medCUMs
model <- medCUMsModel
M <- ncol(data)
N <- nrow(data)
names <- names(data)
actionables <- names(data)[!(names(data) %in% nonactionables)]
actions <- data.frame(matrix(vector(), 0, M, dimnames=list(c(), names)), stringsAsFactors=T)
optimums <- data.frame(matrix(vector(), 0, M, dimnames=list(c(), names)), stringsAsFactors=T)
actionables <- actionables[!(actionables %in% c(ID))]
data$Wellname <- as.character(data$Wellname)
for (well in 1:N) {
#   well <- 7
stub <- data[well, ]
print(stub$Wellname)
# this is to get min-max of Steen Scruggs & Kerner Carson wells
# name=unlist(strsplit(gsub("-", " ", stub$Wellname),split=" ",fixed=TRUE))[1]
# wellsData <- data[grepl(name,data$Wellname),]
# create optimization wrapper for CUM
opt_gm <- function(x, known.x) { #
val <- stub[[METRIC]]
z <- stub[,-which(names(stub) %in% c(ID,METRIC))]
# copy the data over top of the stub record
for (i in names(x)) {
if (i %in% actionables) {
z[[i]] <- x[[i]]
} else {
z[[i]] <- known.x[[i]]
}
}
# score the data and return the negative
y <- predict(model,newdata = z)
#If these conditions are violated, the function returns a large positive number
# which the search procedure will avoid
#     if(y > val*1.1) return(10^38)
-y
}
# start with given values and replace by mean for actionables
start <- stub[,-which(names(stub) %in% c(ID,METRIC))]
# lower
opt_min <- start
for (i in names(opt_min)) {
opt_min[[i]] <- min(data[[i]],na.rm = T)
}
opt_min
# upper
opt_max <- start
for (i in names(opt_max)) {
opt_max[[i]] <- max(data[[i]],na.rm = T)
}
opt_max
opt_start <- start
for (i in names(opt_start)) {
if (i %in% actionables) {
opt_start[[i]] <- mean(data[[i]],na.rm = T)
}
}
opt_start
opt_nons <- start
for (i in names(opt_nons)) {
if (i %in% actionables)
opt_nons[[i]] <- NA
}
opt_nons
opt_gm(opt_start,opt_nons)
# optimize
opt_results <- optim(opt_start, opt_gm, method="L-BFGS-B", control = list(trace=0,maxit=1000,REPORT=1),lower=opt_min, upper=opt_max,known.x=opt_nons) # ,
#   opt_results <- nmkb(opt_start, opt_gm, lower=opt_min, upper=opt_max,known.x=opt_nons) # result ~
# view the optimized inputs & predicted output (CUM)
opt_results
# test the optima
temp <- stub
action <- stub
for(i in names(opt_results$par)) {
opti <- opt_results$par[[i]]
ori <- temp[[i]]
action[[i]] <- "NA"
if (ori != 0)
action[[i]] <- ((opti - ori)/ori)*100
temp[[i]] <-  opti
}
temp$CumProd <- abs(opt_results$value)
action$CumProd <- ((temp$CumProd - stub$CumProd)/stub$CumProd) * 100
optimums[well, ] <- temp
actions[well, ] <- action
}
write.table(optimums, "prescriptions.csv",sep=",",row.names=FALSE, quote = FALSE)
write.table(actions, "actions.csv",sep=",",row.names=FALSE, quote = FALSE)
# clear the memory!
rm(list=ls(all=TRUE))
gc()
# load the dependencies!
require(gdata)
require(randomForest)
require(RWeka)
require(caret)
require(fmsb)
library(foreach)
library(Metrics)
library(e1071)
library(corrplot)
library(RANN)
library(ipred)
library(plyr)
library(rJava)
library(dfoptim)
library(proxy)
library(googleVis)
library(DMwR)
library(rpart)
# set the work directory
setwd("~/Chevron/DSChallenge/nested-learning/results")
if (!exists('challengeModel', mode = "function"))
source("../challenge-model.R")
# load base_training.csv
base <- read.csv("../base_training.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# remove the Deepest Zone as it will be added below
base <- base[, -which(names(base) %in% c("Deepest_Zone"))]
# split Between Zone into 2
zones=data.frame(do.call(rbind, strsplit(as.vector(base$Between_Zone), split = " --> ", fixed=TRUE)))
names(zones) <- c("Top_Zone", "Deepest_Zone")
base <- cbind(base, zones)
# format Completion.Date
# base$Completion.Date = as.Date(base$Completion.Date, origin = "1900-01-01")
# add Completion.Month
# base$Completion.Month = as.numeric(format(base$Completion.Date, "%m"))
# add Completion.Season
# base$Completion.Season <- time2season(base$Completion.Date, out.fmt = "seasons", type = "default")
write.table(base, "base_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load completions_training.csv with Remarks removed!
completions <- read.csv("../completions_training_removedRemarks.csv", header = T)
# unique completions
uniqueCompletions <- unique( completions )
write.table(uniqueCompletions, "completions_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# load geology_training.csv
geo <- read.csv("../geology_training.csv", header = TRUE, sep = ",")
# mark the handpicked columns
handpicked <- c("CLFK..KHW.","U_SPBR..KHW.","L_SPBR..KHW.","WFMP..KHW.","WFMP_L..KHW.","WFMP_ATB..KH","CLINE..KHW.","STRN..KHW.","ATOK..KHW.","BEND..KHW.","ATOKA_L..KHW","MPLM..KHW.","WDFD..KHW.","DVNN..KHW.")
# mark the interpolated columns
interpolated <- c("CLEARFORK..MAP.","SPRABERRY_U..MAP.","SPRABERRY_L..MAP.","WOLFCAMP..MA","WOLFCAMP_L..","WFMP_ATB..MA","CLINE..MAP.","STRAWN..MAP.","ATOKA..MAP.","BEND..MAP.","ATOKA_L..MAP","MISSISSIPPIA","WOODFORD..MA","DEVINOAN_UNC")
# replace the Handpicked NAs by values from the Interpolated columns
for(i in 1:length(handpicked)) {
print(paste(handpicked[i], interpolated[i], sep = " = "))
geo[[handpicked[i]]][is.na(geo[[handpicked[i]]])] <- geo[[interpolated[i]]][is.na(geo[[handpicked[i]]])]
}
# drop interpolated columns
geo = geo[,-which(names(geo) %in% interpolated)]
write.table(geo, "geology_training_edits.csv", sep=",", row.names=FALSE, quote = FALSE)
# merging the files
temp <- merge(base, uniqueCompletions, by="WellID")
DATA <- merge(temp, geo, by="WellID")
write.table(DATA, "data.csv", sep=",", row.names=FALSE, quote = FALSE)
print(paste("=============== Data merged into one file with", nrow(DATA), "rows and", ncol(DATA), "columns ===============", sep = " "))
# load well wise Completions data
# DATA <- read.csv("../eogcompletions.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# print(paste("=============== Data has", nrow(DATA), "rows and", ncol(DATA), "columns ===============", sep = " "))
str(DATA)
# some tweaking constants
missingThreshold <- 0.8
# transformations <- c("BoxCox", "center", "scale") # ,"bagImpute"
iterations <- 100
VIF_THRESHOLD <- 10
trainSplit <- 0.70
corrCutOff <- 0.70
# imp vars!
ID <- "WellID"
USELESS <- c("Completion.Date")
METRIC <- "EUR_o..Mstb."
outliers <- c("Fluid.Water..Gals.","Acid..Gals.","Gel.x.link..Gals.","Proppant...Total..lbs.","Fluid...Total..lbs.","Fluid.Amount","Propping.Agent.Amount") #"Other..Gals.",
# names(DATA)[names(DATA) == "CUM_180PD_OIL_PROD"] <- METRIC
# replace empty strings and #VALUE! as NAs for all columns
for(i in names(DATA)) {
if (length(which(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!")))) > 0) {
DATA[[i]] <- unknownToNA(x=DATA[[i]], unknown=c("","#VALUE!"))
print(paste(i,"replaced garbage=", any(isUnknown(x=DATA[[i]], unknown = c("","#VALUE!"))), sep = " "))
}
}
# str(DATA)
# remove some unwanted columns
# DATA <- DATA[, -which(names(DATA) %in% USELESS)]
# str(DATA)
# detect the numerical columns to remove outliers
data.numericals <- DATA[, which(names(DATA) %in% outliers)]
str(data.numericals)
for(i in names(data.numericals)) {
quantiles <- quantile(data.numericals[[i]], c(.05, .95), na.rm = TRUE)
if (quantiles[1] >= 0 && quantiles[2] >= 0) {
print(i)
data.numericals[[i]][ data.numericals[[i]] < quantiles[1] ] <- quantiles[1]
data.numericals[[i]][ data.numericals[[i]] > quantiles[2] ] <- quantiles[2]
}
}
data.others <- DATA[ , -which(names(DATA) %in% outliers)]
str(data.others)
data.cleaned <- cbind(data.numericals, data.others)
write.table(data.cleaned, "data_edits1.csv", sep=",", row.names=FALSE, quote = FALSE)
str(data.cleaned)
# data.cleaned <- DATA
# set 10% missing threshold
cols <- ncol(data.cleaned)
rows <- nrow(data.cleaned)
NA_RowWise <- apply(data.cleaned, 1, function(z) sum(is.na(z)))
hist(NA_RowWise)
quantile(NA_RowWise)
# eliminate rows whose missing > 20%
colsMissing <- floor(cols*missingThreshold)
colsMissing
DelRows <- which(NA_RowWise > colsMissing)
length(DelRows)
data.rowsCleaned <- data.cleaned
if (length(DelRows) > 0) {
data.rowsCleaned <- data.cleaned[-DelRows,]
}
nrow(data.rowsCleaned)
ncol(data.rowsCleaned)
NA_ColWise <- apply(data.rowsCleaned, 2, function(z) sum(is.na(z)))
hist(NA_ColWise)
quantile(NA_ColWise)
#eliminate cols whose missing > 20%
rowsMissing <- floor(rows*missingThreshold)
rowsMissing
DelCols <- which(NA_ColWise > rowsMissing)
length(DelCols)
data.colsCleaned <- data.rowsCleaned
if (length(DelCols) > 0) {
data.colsCleaned <- data.rowsCleaned[,-DelCols]
}
nrow(data.colsCleaned)
ncol(data.colsCleaned)
write.table(data.colsCleaned, "data_cleaned2.csv", sep=",", row.names=FALSE, quote = FALSE)
# make data point to the cleaned up version
DATA <- data.colsCleaned
# hist(x = DATA$EUR_o..Mstb., plot = T)
# divide into training and test sets
testing <- subset(DATA, is.na(DATA$EUR_o..Mstb.))
data <- subset(DATA, !is.na(DATA$EUR_o..Mstb.))
# category labeling
types <- lapply(data, class)
distincts <- lapply(data, function(c) unique(c))
for(i in names(data)){
noOfCats <- length(levels(distincts[[i]]))
if (noOfCats == 1) {
data[[i]] <- NULL
} else if ((noOfCats <= length(data[[i]])/2 && types[[i]] == "factor") || types[[i]] == "character") {
print(paste("processing ====", i, sep = "> "))
means <- sapply(split(data$EUR_o..Mstb., data[[i]]), function(x) mean(x, na.rm=TRUE))
print(names(means))
# convert to character type
data[[i]] <- as.character(data[[i]])
for(j in names(means)) {
print(paste("replacing", j, "by", means[[j]], sep = " "))
data[[i]][data[[i]] == j] <- means[[j]]
}
data[[i]] <- as.numeric(data[[i]])
}
}
LOW <- 0.65
HIGH <- 0.98
marks1 <- as.numeric(quantile(data$EUR_o..Mstb., c(0, LOW, HIGH, 1)))
marks1
copy <- data
# copy$Class <- cut(x = copy$EUR_o..Mstb., breaks = marks2, include.lowest = T, labels = c(1,2,3,4,5)) # labels = FALSE,
copy$Class <- cut(x = copy$EUR_o..Mstb., breaks = marks1, include.lowest = T, labels = F)
# copy$Class <- factor(copy$Class)
# str(copy)
table(copy$Class)
LOW <- 0.55
HIGH <- 0.96
marks1 <- as.numeric(quantile(data$EUR_o..Mstb., c(0, LOW, HIGH, 1)))
marks1
copy <- data
# copy$Class <- cut(x = copy$EUR_o..Mstb., breaks = marks2, include.lowest = T, labels = c(1,2,3,4,5)) # labels = FALSE,
copy$Class <- cut(x = copy$EUR_o..Mstb., breaks = marks1, include.lowest = T, labels = F)
# copy$Class <- factor(copy$Class)
# str(copy)
table(copy$Class)
temp <- copy[,-which(names(copy) %in% c(METRIC))]
fitOri <- challengeModel(temp,F,T)
pred <- predict(fitOri, type="class",newdata = copy)
cm <- confusionMatrix(pred, copy$Class)
cm
sensitivity <- mean(cm$byClass[1:3,1])
sensitivity
accuracy <- mean(cm$byClass[1:3,8])
accuracy
set.seed(384)
t <- SMOTE(Class ~ ., data = temp, k = 5) #, perc.over = 300, perc.under = 300, k = 5, learner = "rpartXse", se = 0.5
table(t$Class)
fitSMOTE <- challengeModel(t,F,T)
pred <- predict(fitSMOTE, type="class",newdata = copy)
cm <- confusionMatrix(pred, copy$Class)
cm
set.seed(384)
t <- SMOTE(Class ~ ., data = temp, k = 5)
